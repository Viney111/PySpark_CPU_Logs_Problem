{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/24 13:05:26 WARN Utils: Your hostname, viney-Lenovo-G50-80 resolves to a loopback address: 127.0.1.1; using 192.168.0.196 instead (on interface wlp3s0)\n",
      "22/05/24 13:05:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/05/24 13:05:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "@Author: Viney Khaneja\n",
    "@Date: 2022-05-21 09:13\n",
    "@Last Modified by: Viney Khaneja\n",
    "@Last Modified time: None\n",
    "@Title : CPU Logs PFiles Loading from HDFS using PySpark DataFrame\n",
    "'''\n",
    "\n",
    "from pyspark.sql import *\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.config(\"spark.jars\", \"/usr/share/java/mysql-connector-java-8.0.29.jar\").master(\"local\").appName(\"PySpark_MySQL_test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CPU Log Files from Hadoop in Pyspark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd_cpu_logs_data_hdfs = spark.read.csv(\"hdfs://localhost:9000/cpulogs/*.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/24 13:07:03 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "df1 = rdd_cpu_logs_data_hdfs.select([\"user_name\",\"DateTime\",\"keyboard\",\"mouse\",'Cpu Count'])\n",
    "df2 = df1.groupBy(\"user_name\").count()\n",
    "rdd_cpu_logs_data_hdfs.createOrReplaceTempView(\"tbl_working_counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------+------+---------+\n",
      "|           user_name|           DateTime|keyboard| mouse|Cpu Count|\n",
      "+--------------------+-------------------+--------+------+---------+\n",
      "|  iamnzm@outlook.com|2019-09-19 08:40:02|     1.0|  32.0|        2|\n",
      "|  iamnzm@outlook.com|2019-09-19 08:45:02|     0.0|   0.0|        2|\n",
      "|  iamnzm@outlook.com|2019-09-19 08:50:01|     0.0|   0.0|        2|\n",
      "|  iamnzm@outlook.com|2019-09-19 08:55:01|    11.0| 900.0|        2|\n",
      "|  iamnzm@outlook.com|2019-09-19 09:00:01|     2.0|  25.0|        2|\n",
      "|  iamnzm@outlook.com|2019-09-19 09:05:01|    37.0| 336.0|        2|\n",
      "|deepshukla292@gma...|2019-09-19 09:05:01|     0.0|  55.0|        2|\n",
      "|  iamnzm@outlook.com|2019-09-19 09:10:01|     0.0| 136.0|        2|\n",
      "|deepshukla292@gma...|2019-09-19 09:10:01|     6.0|1112.0|        2|\n",
      "|  iamnzm@outlook.com|2019-09-19 09:15:02|     0.0|  84.0|        2|\n",
      "|deepshukla292@gma...|2019-09-19 09:15:02|     0.0| 357.0|        2|\n",
      "|markfernandes66@g...|2019-09-19 09:15:01|    20.0| 670.0|        4|\n",
      "|markfernandes66@g...|2019-09-19 09:10:01|    17.0|   0.0|        4|\n",
      "|markfernandes66@g...|2019-09-19 09:20:01|    29.0|1895.0|        4|\n",
      "|  iamnzm@outlook.com|2019-09-19 09:20:02|     0.0|  29.0|        2|\n",
      "|deepshukla292@gma...|2019-09-19 09:20:02|    35.0|3022.0|        2|\n",
      "|markfernandes66@g...|2019-09-19 09:25:01|     2.0| 238.0|        4|\n",
      "|  iamnzm@outlook.com|2019-09-19 09:25:01|     0.0|   0.0|        2|\n",
      "|deepshukla292@gma...|2019-09-19 09:25:01|    20.0|1330.0|        2|\n",
      "|markfernandes66@g...|2019-09-19 09:30:01|     5.0|  39.0|        4|\n",
      "+--------------------+-------------------+--------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hdfs_pyspark_df = rdd_cpu_logs_data_hdfs.select([\"user_name\",\"DateTime\",\"keyboard\",\"mouse\",'Cpu Count'])\n",
    "hdfs_pyspark_df.show()\n",
    "hdfs_pyspark_df.createOrReplaceTempView(\"tbl_sql_try\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpulogdata\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "database = os.getenv('MYSQL_DB_NAME')\n",
    "user= os.getenv('MYSQL_DB_USER')\n",
    "password = os.getenv('MYSQL_PASSWORD')\n",
    "host= os.getenv('MYSQL_HOST')\n",
    "print(database)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.\n",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to MySql\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    jdb_curl = f\"jdbc:mysql://localhost/\"+database\n",
    "    hdfs_pyspark_df.write.format(\"jdbc\").option(\"url\",jdb_curl) \\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .option(\"dbtable\",\"tbl_sql_try\") \\\n",
    "        .option(\"driver\",\"com.mysql.jdbc.Driver\")\\\n",
    "        .option(\"user\",user).option(\"password\",password).save()\n",
    "    print(\"Data uploaded to MySql\")\n",
    "except Exception as ex:\n",
    "    print(ex)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
