{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 09:31:31 WARN Utils: Your hostname, viney-Lenovo-G50-80 resolves to a loopback address: 127.0.1.1; using 192.168.0.198 instead (on interface wlp3s0)\n",
      "22/05/21 09:31:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/05/21 09:31:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "@Author: Viney Khaneja\n",
    "@Date: 2022-05-21 09:13\n",
    "@Last Modified by: Viney Khaneja\n",
    "@Last Modified time: None\n",
    "@Title : CPU Logs PFiles Loading from HDFS using PySpark DataFrame\n",
    "'''\n",
    "\n",
    "from pyspark.sql import *\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.config(\"spark.jars\", \"/usr/share/java/mysql-connector-java-8.0.29.jar\").master(\"local\").appName(\"PySpark_MySQL_test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CPU Log Files from Hadoop in Pyspark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd_cpu_logs_data_hdfs = spark.read.csv(\"hdfs://localhost:9000/cpulogs/*.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 09:31:54 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "df1 = rdd_cpu_logs_data_hdfs.select([\"user_name\",\"DateTime\",\"keyboard\",\"mouse\",'Cpu Count'])\n",
    "df2 = df1.groupBy(\"user_name\").count()\n",
    "rdd_cpu_logs_data_hdfs.createOrReplaceTempView(\"tbl_working_counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------+------+---------+\n",
      "|           user_name|           DateTime|keyboard| mouse|Cpu Count|\n",
      "+--------------------+-------------------+--------+------+---------+\n",
      "|  iamnzm@outlook.com|2019-09-19 08:40:02|     1.0|  32.0|        2|\n",
      "|  iamnzm@outlook.com|2019-09-19 08:45:02|     0.0|   0.0|        2|\n",
      "|  iamnzm@outlook.com|2019-09-19 08:50:01|     0.0|   0.0|        2|\n",
      "|  iamnzm@outlook.com|2019-09-19 08:55:01|    11.0| 900.0|        2|\n",
      "|  iamnzm@outlook.com|2019-09-19 09:00:01|     2.0|  25.0|        2|\n",
      "|  iamnzm@outlook.com|2019-09-19 09:05:01|    37.0| 336.0|        2|\n",
      "|deepshukla292@gma...|2019-09-19 09:05:01|     0.0|  55.0|        2|\n",
      "|  iamnzm@outlook.com|2019-09-19 09:10:01|     0.0| 136.0|        2|\n",
      "|deepshukla292@gma...|2019-09-19 09:10:01|     6.0|1112.0|        2|\n",
      "|  iamnzm@outlook.com|2019-09-19 09:15:02|     0.0|  84.0|        2|\n",
      "|deepshukla292@gma...|2019-09-19 09:15:02|     0.0| 357.0|        2|\n",
      "|markfernandes66@g...|2019-09-19 09:15:01|    20.0| 670.0|        4|\n",
      "|markfernandes66@g...|2019-09-19 09:10:01|    17.0|   0.0|        4|\n",
      "|markfernandes66@g...|2019-09-19 09:20:01|    29.0|1895.0|        4|\n",
      "|  iamnzm@outlook.com|2019-09-19 09:20:02|     0.0|  29.0|        2|\n",
      "|deepshukla292@gma...|2019-09-19 09:20:02|    35.0|3022.0|        2|\n",
      "|markfernandes66@g...|2019-09-19 09:25:01|     2.0| 238.0|        4|\n",
      "|  iamnzm@outlook.com|2019-09-19 09:25:01|     0.0|   0.0|        2|\n",
      "|deepshukla292@gma...|2019-09-19 09:25:01|    20.0|1330.0|        2|\n",
      "|markfernandes66@g...|2019-09-19 09:30:01|     5.0|  39.0|        4|\n",
      "+--------------------+-------------------+--------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hdfs_pyspark_df = rdd_cpu_logs_data_hdfs.select([\"user_name\",\"DateTime\",\"keyboard\",\"mouse\",'Cpu Count'])\n",
    "hdfs_pyspark_df.show()\n",
    "hdfs_pyspark_df.createOrReplaceTempView(\"tbl_sql_try\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['DB_NAME'] = \"cpulogdata\"\n",
    "os.environ['DB_USER'] = \"root\"\n",
    "os.environ['HOST'] = \"localhost\"\n",
    "os.environ[\"PASSWORD\"] = \"7206594149@Vi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpulogdata\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "database = os.getenv('DB_NAME')\n",
    "user= os.getenv('DB_USER')\n",
    "password = os.getenv('PASSWORD')\n",
    "host= os.getenv('HOST')\n",
    "print(database)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o93.save.\n",
      ": java.sql.SQLSyntaxErrorException: Incorrect column name 'Usage Cpu Count '\n",
      "\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120)\n",
      "\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\n",
      "\tat com.mysql.cj.jdbc.StatementImpl.executeUpdateInternal(StatementImpl.java:1335)\n",
      "\tat com.mysql.cj.jdbc.StatementImpl.executeLargeUpdate(StatementImpl.java:2085)\n",
      "\tat com.mysql.cj.jdbc.StatementImpl.executeUpdate(StatementImpl.java:1246)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.executeStatement(JdbcUtils.scala:1031)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.createTable(JdbcUtils.scala:917)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:80)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    jdb_curl = f\"jdbc:mysql://localhost/\"+database\n",
    "    hdfs_pyspark_df.write.format(\"jdbc\").option(\"url\",jdb_curl) \\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .option(\"dbtable\",\"tbl_sql_try\") \\\n",
    "        .option(\"driver\",\"com.mysql.jdbc.Driver\")\\\n",
    "        .option(\"user\",user).option(\"password\",password).save()\n",
    "    print(\"Data uploaded to MySql\")\n",
    "except Exception as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
